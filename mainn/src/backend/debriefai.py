# -*- coding: utf-8 -*-
"""debriefAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12xOWaoz6CAKc2rove1JBdvwxL1d6lWUx

##sample

prerocessing data
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import joblib

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/flight_lstm_data.csv")

# Check for missing values
print(df.isnull().sum())

# Select columns to scale
columns_to_scale = ["Altitude_ft", "Airspeed_knots", "Vertical_Speed_ftmin",
                    "Pitch_Angle_deg", "Roll_Angle_deg", "Engine_Power_%"]

# Save the original min/max values before scaling
original_min = df["Vertical_Speed_ftmin"].min()
original_max = df["Vertical_Speed_ftmin"].max()

# Normalize flight parameters
scaler = MinMaxScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

# Save the scaler for later use
joblib.dump(scaler, "scaler.pkl")

print(df.head())  # Check the preprocessed data

# Convert 300 and -300 ft/min into normalized values using the saved original values
#climb_threshold = scaler.transform([[0, 0, 500, 0, 0, 0]])[0][2]  # Get the scaled value for 300 ft/min
#descent_threshold = scaler.transform([[0, 0, -500, 0, 0, 0]])[0][2]  # Get the scaled value for -300 ft/min

"""Identify Flight Regimes"""

# Create DataFrame with the same column names for transformation
sample_df = pd.DataFrame([[0, 0, 500, 0, 0, 0]], columns=columns_to_scale)
climb_threshold = scaler.transform(sample_df)[0][2]

sample_df = pd.DataFrame([[0, 0, -500, 0, 0, 0]], columns=columns_to_scale)
descent_threshold = scaler.transform(sample_df)[0][2]


def classify_flight_regime(row):
    if row["Vertical_Speed_ftmin"] > climb_threshold:
        return "Climb"
    elif row["Vertical_Speed_ftmin"] < descent_threshold:
        return "Descent"
    else:
        return "Cruise"

df["Flight_Regime"] = df.apply(classify_flight_regime, axis=1)

print(df["Flight_Regime"].value_counts())  # Verify if we now get Climb and Descent

"""Convert Data into Sequences for LSTM"""

import numpy as np
#Create sequences for LSTM
def create_sequences(data, sequence_length=50):
    sequences = []
    for i in range(len(data) - sequence_length):
        seq = data[i : i + sequence_length]
        sequences.append(seq)
    return np.array(sequences)

# Select only numerical columns for LSTM input
features = df[columns_to_scale].values
sequences = create_sequences(features)

print("Shape of sequences:", sequences.shape)  # (num_samples, sequence_length, num_features)

"""##data collection and preprocessing"""

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/train_dataset/NGAFID_MC_C37.csv', nrows=56)
print(df.columns)
print(len(df.columns))  # Check the total count

import pandas as pd

chunk_size = 10000  # Number of rows per chunk
for chunk in pd.read_csv('/content/drive/MyDrive/train_dataset/NGAFID_MC_C37.csv', chunksize=chunk_size):
    # Process each chunk
    print(chunk)

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/train_dataset/NGAFID_MC_C28.csv', nrows=56)
print(df.columns)
print(len(df.columns))  # Check the total count

import pandas as pd

chunk_size = 10000  # Number of rows per chunk
for chunk in pd.read_csv('/content/drive/MyDrive/train_dataset/NGAFID_MC_C28.csv', chunksize=chunk_size):
    # Process each chunk
    print(chunk)

import pandas as pd

# Load datasets
df1 = pd.read_csv("/content/drive/MyDrive/train_dataset/NGAFID_MC_C28.csv")  # First CSV
df2 = pd.read_csv("/content/drive/MyDrive/train_dataset/NGAFID_MC_C37.csv")  # Second CSV

# Merge datasets (change axis=0 or axis=1 based on your need)
df = pd.concat([df1, df2], axis=0, ignore_index=True)  # Merge by stacking rows

# Select relevant columns
selected_columns = [
    "Altitude_ft", "Airspeed_knots", "Vertical_Speed_ftmin",
    "Pitch_Angle_deg", "Roll_Angle_deg", "Heading_deg",
    "Engine_Power_%", "E1 RPM", "E1 OilP", "E1 OilT", "E1 FFlow",
    "OAT", "NormAc", "IAS", "AltMSL", "VSpd", "FQtyL", "FQtyR",
    "plane_id", "split", "WOW", "Latitude", "Longitude"
]

# Check for missing columns
available_columns = [col for col in selected_columns if col in df.columns]
missing_columns = [col for col in selected_columns if col not in df.columns]

if missing_columns:
    print("Warning: The following columns are missing:", missing_columns)

df_reduced = df[available_columns]

# Reduce dataset to 1000 rows (randomly)
df_reduced_10 = df_reduced.sample(n=10, random_state=42)

# Save to CSV
df_reduced_10.to_csv("small_flight_data.csv", index=False)
print("Reduced dataset saved! Shape:", df_reduced_10.shape)

import pandas as pd

use_columns = [
    "Altitude_ft", "Airspeed_knots", "Vertical_Speed_ftmin",
    "Pitch_Angle_deg", "Roll_Angle_deg", "Heading_deg",
    "Engine_Power_%", "E1 RPM", "E1 OilP", "E1 OilT", "E1 FFlow",
    "OAT", "NormAc", "IAS", "AltMSL", "VSpd", "FQtyL", "FQtyR",
    "plane_id", "split", "WOW", "Latitude", "Longitude"
]

# Load in very small chunks
chunk_size = 1000
df_chunks = pd.read_csv("/content/drive/MyDrive/train_dataset/NGAFID_MC_C28.csv", usecols=use_columns, chunksize=chunk_size)

df_list = []
for chunk in df_chunks:
    chunk = chunk.astype("float16", errors="ignore")  # Reduce memory usage
    df_list.append(chunk)

df_reduced = pd.concat(df_list[:10], axis=0)  # Use only 10,000 rows to save RAM
df_reduced.to_csv("small_flight_data.csv", index=False)

print(" Small dataset saved! Shape:", df_reduced.shape)

import pandas as pd

# Define the available columns (common in both CSVs)
use_columns = [
    "volt1", "volt2", "amp1", "amp2", "FQtyL", "FQtyR",
    "E1 FFlow", "E1 OilT", "E1 OilP", "E1 RPM",
    "OAT", "IAS", "VSpd", "NormAc", "AltMSL",
    "plane_id", "split"
]

# Read both CSVs in chunks to avoid memory crashes
chunk_size = 5000

df_chunks1 = pd.read_csv("/content/drive/MyDrive/debrief/NGAFID_MC_C28-001.csv", usecols=use_columns, chunksize=chunk_size)
df_chunks2 = pd.read_csv("/content/drive/MyDrive/debrief/NGAFID_MC_C37.csv", usecols=use_columns, chunksize=chunk_size)

# Convert chunks into full DataFrames
df1 = pd.concat(df_chunks1, ignore_index=True)
df2 = pd.concat(df_chunks2, ignore_index=True)

# Merge both datasets
df_combined = pd.concat([df1, df2], axis=0)

# Reduce to 1000 rows randomly
df_reduced_1000 = df_combined.sample(n=1000, random_state=42)

# Save to CSV
df_reduced_1000.to_csv("small_flight_data.csv", index=False)
print("Reduced dataset saved with shape:", df_reduced_1000.shape)

"""##start here"""

import pandas as pd

# Define the dataset path
file_path1 = "/content/drive/MyDrive/train_dataset/NGAFID_MC_C28.csv"
file_path2 = "/content/drive/MyDrive/train_dataset/NGAFID_MC_C37.csv"

# Define important columns available in the dataset
selected_columns = [
    "AltMSL", "IAS", "VSpd", "NormAc", "FQtyL", "FQtyR",
    "E1 FFlow", "E1 OilT", "E1 OilP", "E1 RPM", "OAT",
    "plane_id", "split"
]

# Load a small sample from each CSV
chunk_size = 5000  # Load in small parts to avoid memory issues
df_sample1 = pd.read_csv(file_path1, usecols=selected_columns, nrows=500)
df_sample2 = pd.read_csv(file_path2, usecols=selected_columns, nrows=500)

# Combine both samples
df_small = pd.concat([df_sample1, df_sample2], ignore_index=True)

# Save the reduced dataset
df_small.to_csv("sample_flight_data.csv", index=False)
print("Sample dataset saved with shape:", df_small.shape)

"""check for missing value"""

print(df_small.isnull().sum())  # Check for NaN values
df_small.dropna(inplace=True)   # Remove rows with missing values (if any)

"""Removing Missing Rows"""

df_small.dropna(inplace=True)

"""Normalize data"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_small), columns=df_small.columns)

"""converting data into sequence"""

import numpy as np

def create_sequences(data, sequence_length=50):
    sequences = []
    for i in range(len(data) - sequence_length):
        seq = data[i : i + sequence_length]
        sequences.append(seq)
    return np.array(sequences)

sequence_length = 50  # Choose a sequence length (e.g., past 50 time steps)
features = df_scaled.values
sequences = create_sequences(features, sequence_length)

print("LSTM input shape:", sequences.shape)  # (samples, sequence_length, features)

"""test train split"""

train_size = int(len(sequences) * 0.8)  # 80% for training
train_data = sequences[:train_size]
test_data = sequences[train_size:]

print("Training Data Shape:", train_data.shape)
print("Testing Data Shape:", test_data.shape)

"""##LSTM model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense

model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(50, 13)),  # First LSTM keeps sequences
    Dropout(0.2),
    LSTM(50, return_sequences=False),  # Last LSTM should NOT return sequences
    Dense(13)  # Matches y_train shape (None, 13)
])

model.compile(loss='mse', optimizer='adam')
model.summary()

"""train the model"""

X_train = train_data  # Replace with actual variable name
X_test = test_data    # Replace with actual variable name

# Check shapes
print("Training Data Shape:", X_train.shape)
print("Testing Data Shape:", X_test.shape)

y_train = X_train[:, -1, :]  # Last timestep as target
y_test = X_test[:, -1, :]

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

model.save("trained_lstm_model.h5")

"""##Evaluate"""

test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")

y_pred = model.predict(X_test)

import matplotlib.pyplot as plt

# Select a sample index (e.g., first test sample)
sample_idx = 188
sample_idx = 18
# Plot actual vs predicted for the first feature
plt.plot(y_test[sample_idx], label="Actual", marker='o')
plt.plot(y_pred[sample_idx], label="Predicted", marker='x')
plt.xlabel("Flight Parameters")
plt.ylabel("Value")
plt.legend()
plt.title("Actual vs Predicted Flight Parameters")
plt.show()

y_pred = model.predict(X_test)

"""residuals"""

import numpy as np

residuals = np.abs(y_test - y_pred)  # Absolute difference

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(residuals, marker='o', linestyle='-', alpha=0.5)  # Scatter plot
plt.axhline(y=np.mean(residuals) + 3*np.std(residuals), color='r', linestyle='--', label="Anomaly Threshold")
plt.xlabel("Time Step")
plt.ylabel("Residual Error")
plt.title("Residual Errors for Anomaly Detection")
plt.legend()
plt.show()

threshold = np.mean(residuals) + 3 * np.std(residuals)
anomalies = np.where(residuals > threshold)

print("Anomalous points:", anomalies)

plt.figure(figsize=(12, 6))
plt.plot(residuals, marker='o', linestyle='', alpha=0.5, label="Residual Errors")  # Scatter plot

# Highlight anomalies
plt.scatter(anomalies[0], residuals[anomalies], color='red', label="Anomalies", edgecolors='k')

plt.axhline(y=threshold, color='r', linestyle='--', label="Anomaly Threshold")
plt.xlabel("Time Step")
plt.ylabel("Residual Error")
plt.title("Detected Anomalies in Residual Errors")
plt.legend()
plt.show()

"""##testing"""

from tensorflow.keras.models import load_model

model = load_model("/content/drive/MyDrive/trained_lstm_model.h5", compile=False)
print(model.summary())  # Check if the model is loaded correctly

# Convert DataFrame to NumPy array
new_flight_data = new_flight_data.to_numpy()

# Reshape the data
new_flight_data = new_flight_data.reshape(-1, sequence_length, num_features)

# Make predictions
predictions = model.predict(new_flight_data)

residual_errors = np.abs(y_test - y_pred)

threshold = np.mean(residual_errors) + 3 * np.std(residual_errors)

anomalies = np.where(residual_errors > threshold)
print("Anomalous points:", anomalies)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(residual_errors, label="Residual Error")
plt.axhline(y=threshold, color='r', linestyle='--', label="Anomaly Threshold")
plt.scatter(anomalies[0], residual_errors[anomalies], color='red', label="Anomalies")
plt.xlabel("Time Step")
plt.ylabel("Error")
plt.legend()
plt.show()

"""##Flask"""

!pip install flask flask_cors scipy pyngrok

!ngrok authtoken 2uWpt9XWUJ1uX90BUSlS7fMEqLY_6HgvF3tWhPxHBBVnNbUzx

import os

UPLOAD_FOLDER = "/content/uploads"

# Check if the upload folder exists and list its files
if os.path.exists(UPLOAD_FOLDER):
    print("Uploaded files:", os.listdir(UPLOAD_FOLDER))
else:
    print("Upload folder does not exist!")

pip install flask-cors

from flask import Flask, request, jsonify
from flask_cors import CORS
import os
from werkzeug.utils import secure_filename
from pyngrok import ngrok  # To expose Colab server

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})


UPLOAD_FOLDER = "/content/uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

@app.route("/upload", methods=["POST"])
def upload_file():
    if "file" not in request.files:
        return jsonify({"message": "No file uploaded"}), 400

    file = request.files["file"]
    filename = secure_filename(file.filename)
    file_path = os.path.join(UPLOAD_FOLDER, filename)
    file.save(file_path)

    return jsonify({"message": f"{filename} uploaded successfully!", "path": file_path})

# Start Flask app
port = 5000
public_url = ngrok.connect(port).public_url  # Get public URL
print(f"Public URL: {public_url}")

app.run(port=5000)

import os
print("Uploaded files:", os.listdir("/content/uploads"))

!cat /content/uploads/*

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/upload', methods=['POST'])
def upload():
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
    file = request.files['file']
    return jsonify({'filename': file.filename, 'status': 'Uploaded successfully'})

if __name__ == '__main__':
    app.run(debug=True)

